# Copyright 2024 Canonical Ltd.  This software is licensed under the
# GNU Affero General Public License version 3 (see the file LICENSE).

from base64 import b64encode
from datetime import datetime, timezone
from typing import Any, NamedTuple
import uuid

import pytest
from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncConnection
from temporalio import activity
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker

from maasservicelayer.db import Database
from maasservicelayer.db.tables import NodeTagTable, TagTable
from maasservicelayer.models.bmc import Bmc
from maasservicelayer.models.users import User
from maasservicelayer.services import CacheForServices
from maastemporalworker.workflow.tag_evaluation import (
    EVALUATE_TAG_ACTIVITY_NAME,
    TagEvaluationActivity,
    TagEvaluationParam,
    TagEvaluationResult,
    TagEvaluationWorkflow,
)
from metadataserver.enum import SCRIPT_STATUS
from provisioningserver.refresh.node_info_scripts import (
    LLDP_OUTPUT_NAME,
    LSHW_OUTPUT_NAME,
)
from tests.fixtures.factories.bmc import create_test_bmc
from tests.fixtures.factories.machines import create_test_machine_entry
from tests.fixtures.factories.scriptresult import (
    create_test_scriptresult_entry,
)
from tests.fixtures.factories.scriptset import create_test_scriptset_entry
from tests.fixtures.factories.tag import create_test_tag_entry
from tests.fixtures.factories.user import create_test_user
from tests.maasapiserver.fixtures.db import Fixture


class _TestMachineExtraDetails(NamedTuple):
    current_commissioning_script_set_id: int
    script_name: str
    stdout: str


@pytest.fixture
async def _machine_entries_for_tag_evaluation_tests(
    fixture: Fixture,
) -> tuple[
    dict[str, Any],
    dict[str, Any],
    dict[str, Any],
    dict[str, Any],
    dict[str, Any],
]:
    """
    Set up the database to test the tag evaluation in different scenarios.

    The database is configured with 5 machines, where a set of scripts with the
    LSHW_OUTPUT_NAME and LLDP_OUTPUT_NAME scripts ran on them.

    Each machine provides some characteristics that will help to validate the
    result:
    machine 1: machine that ran successfully the LSHW_OUTPUT_NAME script whose
        output states that it is a notebook manufactured by Vendor X
    machine 2: machine that ran successfully the LSHW_OUTPUT_NAME script whose
        output states that it is a server manufactured by Vendor Y
    machine 3: machine that ran successfully the LSHW_OUTPUT_NAME script but
        the output of the script is empty
    machine 4: machine that ran successfully the LSHW_OUTPUT_NAME script but
        the output of the script is malformed
    machine 5: machine that ran successfully the LLDP_OUTPUT_NAME script
    """
    STDOUT_LSHW_VENDOR_X = """<?xml version="1.0" standalone="yes" ?>
<!-- generated by lshw- -->
<node id="latitude" claimed="true" class="system">
  <description>Notebook</description>
  <vendor>Vendor X</vendor>
</node>
"""

    STDOUT_LSHW_VENDOR_Y = """<?xml version="1.0" standalone="yes" ?>
<!-- generated by lshw- -->
<node id="latitude" claimed="true" class="system">
  <description>Server</description>
  <vendor>Vendor Y</vendor>
</node>
"""

    STDOUT_LLDP = """<?xml version="1.0" encoding="UTF-8"?>
<lldp label="LLDP neighbors"/>
"""

    user = await create_test_user(fixture)
    bmc = await create_test_bmc(fixture)

    machine_1 = await _create_test_machine(
        fixture,
        bmc,
        user,
        _TestMachineExtraDetails(
            current_commissioning_script_set_id=10,
            script_name=LSHW_OUTPUT_NAME,
            stdout=STDOUT_LSHW_VENDOR_X,
        ),
    )

    machine_2 = await _create_test_machine(
        fixture,
        bmc,
        user,
        _TestMachineExtraDetails(
            current_commissioning_script_set_id=15,
            script_name=LSHW_OUTPUT_NAME,
            stdout=STDOUT_LSHW_VENDOR_Y,
        ),
    )

    machine_3 = await _create_test_machine(
        fixture,
        bmc,
        user,
        _TestMachineExtraDetails(
            current_commissioning_script_set_id=18,
            script_name=LSHW_OUTPUT_NAME,
            stdout="",
        ),
    )

    machine_4 = await _create_test_machine(
        fixture,
        bmc,
        user,
        _TestMachineExtraDetails(
            current_commissioning_script_set_id=23,
            script_name=LSHW_OUTPUT_NAME,
            stdout="malformed xml document",
        ),
    )

    machine_5 = await _create_test_machine(
        fixture,
        bmc,
        user,
        _TestMachineExtraDetails(
            current_commissioning_script_set_id=29,
            script_name=LLDP_OUTPUT_NAME,
            stdout=STDOUT_LLDP,
        ),
    )

    return machine_1, machine_2, machine_3, machine_4, machine_5


async def _create_test_machine(
    fixture: Fixture,
    bmc: Bmc,
    user: User,
    extra_details_test_machine: _TestMachineExtraDetails,
) -> dict[str, Any]:
    """
    Create a machine entry for the tag evaluation tests.

    The test machines need to be linked to a script set which is linked at least
    one script. This information is registered in the database by providing the
    information in the extra_details_test_machine argument.

    Note that this machine is a utility used in the fixture
    _machine_entries_for_tag_evaluation_tests defined above.
    """
    extra_details_machine = {
        "current_commissioning_script_set_id": extra_details_test_machine.current_commissioning_script_set_id
    }
    machine = await create_test_machine_entry(
        fixture, bmc=bmc, user=user, **extra_details_machine
    )

    extra_details_scriptset = {
        "id": extra_details_test_machine.current_commissioning_script_set_id
    }
    scriptset = await create_test_scriptset_entry(
        fixture, node_id=machine["id"], **extra_details_scriptset
    )

    extra_details_scriptresult = {
        "script_name": extra_details_test_machine.script_name,
        "stdout": b64encode(
            extra_details_test_machine.stdout.encode("ascii")
        ).decode("ascii"),
    }
    await create_test_scriptresult_entry(
        fixture,
        script_set_id=scriptset["id"],
        status=SCRIPT_STATUS.PASSED,
        **extra_details_scriptresult,
    )

    return machine


@pytest.mark.asyncio
class TestTagEvaluationWorkflow:
    """
    Test class gathering all the tag evaluation workflow tests.
    """

    async def test_tag_evaluation_workflow(self):
        calls = {}
        activity_param = TagEvaluationParam(101, "//node")

        @activity.defn(name=EVALUATE_TAG_ACTIVITY_NAME)
        async def mock_evaluate_tag(
            param: TagEvaluationParam,
        ) -> TagEvaluationResult:
            calls["evaluate-tag"] = param
            return TagEvaluationResult(inserted=0, deleted=0)

        async with await WorkflowEnvironment.start_time_skipping() as env:
            async with Worker(
                env.client,
                task_queue="test::region",
                workflows=[TagEvaluationWorkflow],
                activities=[mock_evaluate_tag],
            ) as worker:
                await env.client.execute_workflow(
                    workflow=TagEvaluationWorkflow.run,
                    arg=activity_param,
                    id=f"workflow-{uuid.uuid4()}",
                    task_queue=worker.task_queue,
                )

        assert calls["evaluate-tag"] == activity_param


@pytest.mark.asyncio
@pytest.mark.usefixtures("maasdb")
class TestTagEvaluationActivities:
    """
    Test class gathering all the tests for tag evaluation activities.
    """

    @pytest.mark.parametrize("batch_size", [1000, 2, 1])
    async def test_tag_evaluation_activity(
        self,
        db: Database,
        db_connection: AsyncConnection,
        fixture: Fixture,
        _machine_entries_for_tag_evaluation_tests,
        batch_size,
    ):
        """
        Test the tag evaluation activity.

        The starting point of the test is the database entries defined by the
        fixture `_machine_entries_for_tag_evaluation_tests`. See the docstring
        of the feature to learn more about the setup.

        This test evaluates the behavior of the tag evaluation activity based on
        the interaction with the database:
        - TEST 1: tag is assigned correctly to a node if there is a node-tag
            match
        - TEST 2: tag is not assigned to a node if there is no node-tag match
        - TEST 3: tag is assigned to a node when the tag definition is updated
            and there is a node-tag match
        - TEST 4: tag is unassigned from a node when the tag definition is
            updated and there is no more a node-tag match

        Machines 3 and 4 are not used directly in the tests but they still
        provide the following validations:
        - TEST 5: tag is not assigned to a node when the output of the
            commissioning script is empty (no error raised)
        - TEST 6: tag is not assigned to a node when the output of the
            commissioning script is malformed (no error raised)

        Additional tests:
        - TEST 7: same as TEST 1 but using the LLDP_OUTPUT_NAME script
        """

        async def _retrieve_node_tag_entries():
            """
            Closure that retrieves the entries of the node-tag "through"
            (many-to-many) table.

            NodeTagTable contains a registry of all the relations between node
            and tags. Indeed, the final outcome of the tag evaluation algorithm
            is to update this table adding or removing entries based on the
            result of the evaluation.
            """
            nodetag_query = select(
                NodeTagTable.c.node_id, NodeTagTable.c.tag_id
            ).select_from(NodeTagTable)

            cursor_result = await db_connection.execute(nodetag_query)
            rows = cursor_result.all()
            return rows

        services_cache = CacheForServices()
        tag_evaluation_activity = TagEvaluationActivity(
            db, services_cache, connection=db_connection
        )

        machine_1, machine_2, _, _, machine_5 = (
            _machine_entries_for_tag_evaluation_tests
        )

        # TEST 1.a: tag is assigned correctly to a node if there is a node-tag
        #   match
        tag_01 = await create_test_tag_entry(
            fixture, name="tag_01", definition="//node"
        )
        param_tag_01 = TagEvaluationParam(
            tag_01["id"], tag_01["definition"], batch_size=batch_size
        )
        result = await tag_evaluation_activity.evaluate_tag(param_tag_01)
        #   tag_01 should match against the output of the script
        #   LSHW_OUTPUT_NAME. Thus, NodeTagTable should have an entry for the
        #   new node-tag relation
        rows = await _retrieve_node_tag_entries()
        assert result == TagEvaluationResult(inserted=2, deleted=0)
        assert len(rows) == 2
        assert set(rows) == {
            (machine_1["id"], tag_01["id"]),
            (machine_2["id"], tag_01["id"]),
        }

        # TEST 2: tag is not assigned to a node if there is no node-tag match
        tag_02 = await create_test_tag_entry(
            fixture, name="tag102", definition="//machine"
        )
        param_tag_02 = TagEvaluationParam(
            tag_02["id"], tag_02["definition"], batch_size=batch_size
        )
        result = await tag_evaluation_activity.evaluate_tag(param_tag_02)
        #   tag_02 should not match against the output of the script
        #   LSHW_OUTPUT_NAME. Thus, NodeTagTable should not have new entries,
        #   remaining as it was
        rows = await _retrieve_node_tag_entries()
        assert result == TagEvaluationResult(inserted=0, deleted=0)
        assert len(rows) == 2
        assert set(rows) == {
            (machine_1["id"], tag_01["id"]),
            (machine_2["id"], tag_01["id"]),
        }

        # TEST 3: tag is assigned to a node when the tag definition is updated
        # and there is a node-tag match
        stmt = (
            update(TagTable)
            .returning(TagTable.c.id, TagTable.c.definition)
            .values(
                updated=datetime.now(timezone.utc).astimezone(),
                definition='//vendor[text()="Vendor X"]',
            )
            .where(TagTable.c.id == tag_02["id"])
        )
        cursor_result = await db_connection.execute(stmt)
        tag_02 = cursor_result.all()[0]
        param_tag_02 = TagEvaluationParam(
            tag_02.id, tag_02.definition, batch_size=batch_size
        )
        result = await tag_evaluation_activity.evaluate_tag(param_tag_02)
        #   tag_02 should match against the output of the script
        #   LSHW_OUTPUT_NAME. Thus, NodeTagTable should have an entry for the
        #   new node-tag relation
        rows = await _retrieve_node_tag_entries()
        assert result == TagEvaluationResult(inserted=1, deleted=0)
        assert len(rows) == 3
        assert set(rows) == {
            (machine_1["id"], tag_01["id"]),
            (machine_2["id"], tag_01["id"]),
            (machine_1["id"], tag_02.id),
        }

        # TEST 4: tag is unassigned from a node when the tag definition is
        # updated and there is no node-tag match anymore
        stmt = (
            update(TagTable)
            .returning(TagTable.c.id, TagTable.c.definition)
            .values(
                updated=datetime.now(timezone.utc).astimezone(),
                definition='//vendor[text()="Vendor Z"]',
            )
            .where(TagTable.c.id == tag_01["id"])
        )
        cursor_result = await db_connection.execute(stmt)
        tag_01 = cursor_result.all()[0]
        param_tag_01 = TagEvaluationParam(
            tag_01.id, tag_01.definition, batch_size=batch_size
        )
        result = await tag_evaluation_activity.evaluate_tag(param_tag_01)
        #   tag_01 should not match anymore the output of the script
        #   LSHW_OUTPUT_NAME. Thus, NodeTagTable should have fewer entries due
        #   to the lots of node-tag relations
        rows = await _retrieve_node_tag_entries()
        assert result == TagEvaluationResult(inserted=0, deleted=2)
        assert len(rows) == 1
        assert set(rows) == {(machine_1["id"], tag_02.id)}

        # TEST 7: same as TEST 1 but using the LLDP_OUTPUT_NAME script
        tag_03 = await create_test_tag_entry(
            fixture, name="tag103", definition="//lldp"
        )
        param_tag_03 = TagEvaluationParam(
            tag_03["id"], tag_03["definition"], batch_size=batch_size
        )
        result = await tag_evaluation_activity.evaluate_tag(param_tag_03)
        #   tag_03 should match against the output of the script
        #   LLDP_OUTPUT_NAME. Thus, NodeTagTable should have an entry for the
        #   new node-tag relation
        rows = await _retrieve_node_tag_entries()

        assert result == TagEvaluationResult(inserted=1, deleted=0)
        assert len(rows) == 2
        assert set(rows) == {
            (machine_1["id"], tag_02.id),
            (machine_5["id"], tag_03["id"]),
        }
